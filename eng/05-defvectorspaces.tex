\chapter{Vector Spaces}
\label{chapter:04vectorspaces}

 

So far, we have established that it isn't too hard to generalize
our notion of \stress{vectors} to include elements of $\R^n$.  We
still call them \stress{vectors} even though we can no longer quite
imagine them as arrows in space.  

As we worked through vector geometry, we also saw
a number of problems coming up, principally among them:
 what are the higher-dimensional
analogues of lines and planes?  How can you describe them?

We will tackle this problem next, but it turns out that the
best way to do this is to step back and see just how far
we can generalize this notion of \stress{vector spaces} --- 
was $\R^n$ the only set that behaves substantially
like $\R^2$ and $\R^3$, or are there many more mathematical
objects out there that, if we look at them in the right
way, are just like vectors, too?

\section{A first example}
So far:  we agree that the elements of  $\R$, $\R^2$ and $\R^3$ are 
\defn{geometric vectors}.

We also agree that we can call elements of $\R^n$, for $n \geq 4$,
\defn{vectors}.

What we are looking for next:  non-geometric, non-$\R^n$ vectors.

\begin{myexample} {\bf Spaces of Equations}

Consider three equations, which we name $E_1$, $E_2$ and $E_3$:
\begin{align*}
E_1 &: \quad  &x-y-z &= -1 \\
E_2 &: \quad &2x-y+z &= 1\\
E_3 &: \quad &-x+2y+4z &=4
\end{align*}
We can create new equations from these ones; for example,
$$
E_4 = E_2-2E_1 \quad : \quad y+3z=3
$$
or
$$
E_1 + E_3 \quad : \quad y+3z = 3
$$
So we can even say
$$
E_2-2E_1 = E_1 + E_3
$$
and it is legitimate to rewrite this as:
$$
3E_1 -E_2 + E_3 = 0
$$
where ``0'' stands for the equation ``$0=0$''.

Well, what are we really saying here?
\begin{itemize}
\item We can {\bf add} two equations to get another equation.
\item We can {\bf multiply} an equation {\bf by a scalar} to get
another equation.
\item There's a {\bf zero equation} given by $0=0$.  Let's call it
$E_0$.
\item Equations have {\bf negatives}:
$$
-E_1 : -x+y+z=1
$$
Why is this the negative?  Because now $(E_1) + (-E_1) = E_0$.
\item The {\bf usual rules of arithmetic} hold:
\begin{itemize}
\item $E_1 + E_2 = E_2+E_1$
\item $E_1 + (E_2+E_3) = (E_1 + E_2) + E_3$
\item $k(E_1+E_2) = kE_1+kE_2$
\item $(k+l)E_1 = kE_1 + lE_1$
\item $(kl)E_1 = k(lE_1)$
\item $1 E_1 = E_1$
\end{itemize}
\end{itemize}


In other words:  these are {\bf exactly} the properties of $\R^n$
that we identified last week!  So even though we have no reasonable
way (yet) of writing ``equations'' as $n$-tuples of numbers, 
algebraically we recognize that they act just like vectors do.
\end{myexample}

 

\standout{Major idea \#1 (this chapter): there are lots of different vector spaces besides $\R^n$.}

We can specialize this a bit more (which also helps us start to
see the value in this approach):

Consider the space $\mathcal{E}$ of all equations ``obtainable from'' (also say:  \stress{generated by} or \stress{spanned by}) $E_1$, $E_2$, and $E_3$:
$$
\mathcal{E} = \{ k_1E_1 + k_2 E_2 + k_3 E_3 \;| \; k_i \in \R\}.
$$
(This is the set of \emph{all} linear combinations \index{linear combination} of those three equations!)
In fact, $\mathcal{E}$ itself acts like a space of vectors (in
the sense above, and to be made precise below).

The kinds of questions you'd want to ask:
Is there an equation of the form $y=y_0$ in $\mathcal{E}$? That is,
can we solve for $y$?\footnote{The 
connection is that: to solve for $y$, what you're honestly
doing is taking linear combinations of equations.}  (Or for $x$, for that matter.) 

\begin{myexample}
Can we find $a,b,y_0\in\R$ so that the equation $y=y_0$
equals $aE_1+bE_2$?  Answer: NO.  Why?  The coefficient
of $x$ in $aE_1+bE_2$ is $a+2b$ and the coefficient of
$z$ in $aE_1+bE_2$ is $-a+b$.  The only way these
can both be zero is if $a$ and $b$ are zero (check);
but then $aE_1+bE_2 = E_0$, the zero equation.  So 
we can't get an equation like $y=y_0$. \end{myexample}

(We of course want to answer tougher questions than that,
but our technique (Gaussian elimination, Chapter~\ref{chapter:11solvingsystems}) will
be based entirely on this idea of taking linear combinations
of equations.)

\defn{Stoichiometry} is the science of figuring out how to
produce a given chemical as a result of reactions of other,
easier to obtain, chemicals.  In stoichiometry, you let $\mathcal{E}$
be the set of all known chemical reactions and want to choose
a best (linear) combination of reactions which will result in
what you wanted.

 
\standout{Major idea \#2: (next two chapters): Subspaces and 
spanning sets.  To answer particular problems, we typically need to
understand the subspace spanned by some vectors.}

\section{So what do we really need?}
Idea:  \stress{vectors} don't have to be geometric, or even $n$-tuples.
But we want things that act like our known examples of vectors
\emph{algebraically}.  (Please note: we're going to ignore the
geometry (like the dot product) for quite a while; we're now
just focussing on the algebra.)

So we need: 
\begin{itemize}
\item $V$ : a set of ``vectors'' of some sort
\item a rule for ``adding'' two ``vectors'' (denoted with $+$)
\item a rule for ``scalar multiplication'' of a ``vector'' by an element $c\in \R$
\end{itemize}
such that all 10 of the following \defn{axioms} hold:
\begin{description}
\item[\it Closure] (These two axioms guarantee that $V$ is ``big enough'')\\
(1) The sum of two vectors should again be a vector.  (That is, if $\uu, \vv \in V$ then $\uu + \vv \in V$.)\\
(2) Any scalar multiple of a vector should again be a vector.  (That is, if $\uu \in V$ and $c\in \R$ then $c\uu \in V$.)
\item[\it Existence] (These two axioms guarantee that   $V$ has the `basics')\\
(3) There should be a zero vector $\zero$ in $V$; it must satisfy
$\zero + \uu = \uu$ for all $\uu \in V$.\\
(4) Every vector in $V$ should have a negative in $V$.  That is, given $\uu \in V$, there
should exist another vector $-\uu \in V$ such that $\uu + (-\uu) = \zero$.
\item[\it Arithmetic properties] (These axioms guarantee that operations behave the way they did in $\R^n$)\\
For any $\uu, \vv, \ww \in V$ and any $c,d\in \R$:\\
(5) $\uu + \vv = \vv + \uu$\\
(6) $\uu + (\vv + \ww) = (\uu+\vv)+\ww$\\
(7) $c(\uu + \vv) = c\uu + c\vv$\\
(8) $(c+d)\uu = c\uu + d\uu$\\
(9) $c(d\uu) = (cd)\uu$\\
(10) $1\uu = \uu$.
\end{description}

\begin{definition}
Any set $V$ with two operations as above, satisfying these 10 axioms is
called a \defn{vector space}.
\end{definition}

Note that even though they are not axioms, it is indeed true that the zero vector is  the result of
scaling any vector by $0 \in \R$, and the negative of a vector
is given by multiplying it by $-1 \in \R$. See problem  \ref{prob04.14}.

\section{Examples of Vector Spaces}

\begin{myexample} $\R$, $\R^2$, $\ldots$, $\R^n$ are all vector spaces, with
the usual operations. \end{myexample}

\begin{myexample} $\mathcal{E}$ is a vector space, as is the set of \stress{all}
linear equations in $n$ variables, with the usual operations. \end{myexample}

\begin{myexample} The set $V = \{\zero\}$, with operations given by the rule 
$\zero + \zero = \zero$, and $c \cdot \zero = \zero$, is a vector space :  the \emph{zero vector
space}.  (But the empty set is not a vector space because it doesn't contain $\zero$!) \end{myexample}

\begin{myexample} The set $V = \{(x,2x) | x \in \R\}$, with the standard operations
from $\R^2$ is a vector space because:
\begin{itemize}
\item CLOSURE (1) Let $\uu, \vv \in V$.  Then $\uu = (x,2x)$ for some
$x\in\R$ and $\vv = (y,2y)$ for some $y\in \R$.  So $\uu + \vv = (x+y, 2x+2y) = (x+y, 2(x+y))$ and this is in $V$ because it is of the form $(z,2z)$ 
with $z = x+y \in \R$.
\item CLOSURE (2) Let $\uu = (x,2x)$ and $c\in \R$.  Then $c\uu = (cx,c(2x))=(cx,2(cx))$ which is again in $V$ because it has the form $(z,2z)$ with $z=cx \in \R$.
\item EXISTENCE (3) The zero vector of $\R^2$ is $(0,0)$; this lies in $V$
and moreover since it satisfies $\zero + \uu = \uu$ for any $\uu \in\R^2$,
it certainly satisfies that for the $\uu \in V$.  So it is a zero vector
and it lies in $V$.
\item EXISTENCE (4) The negative of $\uu = (x,2x)$ is $(-x,-2x) = (-x,2(-x))$
since $(x,2x)+(-x,2(-x))=(0,0)$ and we see that $(-x,2(-x))\in V$.  So
it's the negative and it lies in $V$.
\item ALGEBRAIC PROPERTIES  (5)-(10):  these are all ok because they
all work for ANY $\uu, \vv,\ww$ in $\R^2$, so they certainly work
for any vectors  $\uu, \vv,\ww$  in $V$.
\end{itemize}
So this is a vector space. \end{myexample}

\begin{myexample} The set $V = \{(x,x+2) | x\in \R\}$, with the usual operations of $\R^2$ is NOT A VECTOR SPACE.

To show that $V$ is not a vector space, it is enough to give just ONE
example of just ONE axiom that fails even ONCE.  (Because being
a vector space means that all those axioms are always true.)

But let's look at ALL the axioms, just to see how this fails in 
lots of different ways.
\begin{itemize}
\item CLOSURE (1)  Take $(x,x+2)$ and $(y,y+2)$.  Then their sum is
$(x+y, x+y+4)$ which does NOT have the correct form $(z,z+2)$, for
any $z$.  That is, when you add two elements in this set, you end
up OUTSIDE the set.  It is NOT CLOSED UNDER ADDITION.
\item CLOSURE (2) Take $c\in \R$ and $\uu = (x,x+2)\in V$.  Then $c(x,x+2) = (cx, cx+2c)$ so whenever $c\neq 1$, $c\uu \notin V$.  So $V$ is
NOT CLOSED UNDER SCALAR MULTIPLICATION.
\item EXISTENCE (3) The zero vector $(0,0)$ is not in $V$, since 
you can't have $(0,0) = (z,z+2)$ for any $z\in \R$.  (See exercise 4.1 (h).)
\item EXISTENCE (4) The negative of $\uu = (x,x+2)$ is $-\uu = (-x,-x-2)$,
which doesn't lie in $V$.
\item ALGEBRAIC PROPERTIES (5)-(10): these are all fine, because the
actual operations are fine; it's the subset of $\R^2$ we chose 
that was ``bad''.
\end{itemize} \end{myexample}

\standout{Remember this:  a subset of a vector space can't be a vector
space unless it contains the zero vector.  So lines and planes that don't
pass through the origin are \stress{not vector spaces}.}

\begin{definition}
A \defn{matrix} is a table of numbers, usually enclosed in
square brackets.  It's size is $m \times n$ if it has $m$ rows and
$n$ columns.  For instance, 
$$\mat{1 & 2 & 3 \\ 4 & 5 & 6}$$
is a $2 \times 3$ matrix.
Two matrices of the same size can be added componentwise, like
$$
\mat{1 & 2 \\ 3 & 4} + \mat{5 & 6 \\ 7 & 8} = \mat{6 & 8 \\ 10 & 12}
$$
and we can scalar multiply them componentwise as well, like
$$
2\mat{1 & 2 \\ 3 & 4} = \mat{2 & 4 \\ 6 & 8}.
$$
\end{definition}

\begin{myexample} $\displaystyle V = M_{22}(\R) = \left\{ \mat{a & b \\ c & d} | a,b,c,d \in \R \right\}$, with the above operations.

We check the axioms:  closure is fine (sum and scalar multiples give you 
$2\times 2$ matrices again); existence: (3)  $\mat{0&0 \\ 0 & 0} \in M_{2 2}(\R)$ and clearly adding this to a matrix $A$ gives you $A$ back again,
and (4) $\mat{-a&-b\\-c&-d} \in M_{22}(\R)$ and adding this to
$\mat{a&b\\c&d}$ gives you the zero matrix, so indeed every matrix in
$M_{22}(\R)$ has a negative in $M_{22}(\R)$.  

The algebraic properties (5)-(10) are just like
in $\R^4$; checking them is straightforward.  I include the
proof below, using a few different (acceptable) techniques.  

Begin
by letting $\uu = \mat{u_1 & u_2 \\ u_3 & u_4}$, $\vv = \mat{v_1&v_2\\v_3&v_4}$, etc, and let $c,d\in \R$.  Then
\begin{itemize}
\item (5) $\uu + \vv = \mat{u_1+v_1 & u_2+v_2\\ u_3+v_3 & u_4+v_4}$ and
$\vv + \uu = \mat{v_1+u_1 & v_2+u_2\\ v_3+u_3 & v_4+u_4}$; these are
equal.  So (5) holds.
\item (6) $\uu + (\vv + \ww) = \mat{u_1 & u_2 \\ u_3 & u_4}+ \mat{v_1+w_1 & v_2+w_2\\ v_3+w_3 & v_4+w_4} = \mat{ u_1 + (v_1+w_1) & u_2+(v_2+w_2) \\ u_3 + (v_3+w_3) & u_4+(v_4+w_4)}$ and similarly
$(\uu + \vv) + \ww =  \mat{ (u_1 + v_1)+w_1 & (u_2+v_2)+w_2 \\ (u_3 + v_3)+w_3 & (u_4+v_4)+w_4}$; these are equal.  So (6) holds.
\item (7) 
\begin{align*}
c(\uu + \vv) &= c \mat{u_1+v_1 & u_2+v_2\\ u_3+v_3 & u_4+v_4} \\
&= \mat{c(u_1+v_1) & c(u_2+v_2)\\ c(u_3+v_3) & c(u_4+v_4)} \\
&= \mat{cu_1+cv_1 &  cu_2+cv_2\\ cu_3+cv_3 & cu_4+cv_4}\\
&= \mat{cu_1 & cu_2 \\ cu_3 & cu_4}+ \mat{cv_1&cv_2\\cv_3&cv_4}\\
&= c\uu + c\vv
\end{align*} 
so this axiom holds.
\item (8) \begin{align*}
(c+d)\uu &= (c+d)\mat{u_1 & u_2 \\ u_3 & u_4}\\
&= \mat{(c+d)u_1 & (c+d)u_2 \\ (c+d)u_3 & (c+d)u_4}\\
&= \mat{cu_1+du_1 & cu_2+du_2 \\ cu_3+du_3 & cu_4+du_4}\\
&= \mat{cu_1 & cu_2 \\ cu_3 & cu_4}+\mat{du_1 & du_2 \\ du_3 & du_4}\\
&= c\uu + d\uu
\end{align*}
as required, so this axiom holds.
\item (9) Compare the $i$th component of each side, $1 \leq i \leq 4$.  
The $i$th
entry of $a(b\uu)$ is $a(bu_i)$ and the $i$th entry of $(ab)\uu$ is
$(ab)u_i$, and these are equal in $\R$.  So $a(b\uu) = (ab)\uu$
since each of their entries are equal.
\item (10) The $i$ entry of $1\uu$ is $1u_i = u_i$, which is the
$i$th entry of $\uu$.  So $1\uu = \uu$.
\end{itemize}

So $M_{22}(\R)$ is a vector space.
\end{myexample}

\standout{In fact, $M_{m\,n}(\R)$, the set of all $m\times n$ matrices, when given operations analogous to those described above, is a vector space, for any $m,n \geq 1$.}



Our next example is a space of functions.  We first establish some
notation.

\begin{definition}
Let $[a,b]$ denote the interval $\{x \in \R | a \leq x \leq b\}$, and
let
$$
F[a,b] = \{ f | f \colon [a,b] \to \R\}
$$
be the set of all functions with domain $[a,b]$ with values in $\R$.
For $f,g \in F[a,b]$, we have that $f=g$ if and only if $f(x)=g(x)$
for all $x\in [a,b]$.  Moreover, we define $f+g$ to be the function
which sends $x$ to $f(x)+g(x)$, that is:
$$
(f+g)(x) = f(x) + g(x)
$$
and for any scalar $c \in \R$, $cf$ is the function taking $x$ to
$cf(x)$, that is:
$$
(cf)(x) = c(f(x)).
$$
Geometrically, addition corresponds to vertically adding the
graphs of $f$ and $g$; and scalar multiplication corresponds
to scaling the graph of $f$ by $c$.
\end{definition}

\begin{myexample} {\bf Space of functions}

$F[a,b]$ is a vector space with these operations.

Check:  closure is good; the zero vector is the zero function 
which sends every $x$ to $0$; the negative of $f$ is the
function $-f$ which sends $x$ to $-f(x)$; and the algebraic
operation axioms all hold.

\end{myexample}

\begin{myexample} We could also consider $F(\R)$, the set of all functions
from $\R$ to $\R$, with the same operations; this is also a vector
space.    So for example $\cos(x) \in F(\R)$ and $x+x^2 \in F(\R)$;
in fact all polynomial functions are contained in $F(\R)$.
But $\frac{1}{x}, \tan(x) \notin F(\R)$ because they are not
defined on all of $\R$.   
\end{myexample}

In several of our examples, axioms (5)-(10) were obviously true
because they were true for a larger superset of vectors.  What
this means is that we will have a shortcut for checking if
a subset of a vector space (with the \stress{same operations}) is itself
a vector space.



\section*{Problems}
\addcontentsline{toc}{section}{Problems}
%
% Use the following environment.
% Don't forget to label each problem;
% the label is needed for the solutions' environment


