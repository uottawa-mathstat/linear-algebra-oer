\chapter{Linear Dependence and Independence}\label{Chapter:07independence}



In the last chapter, we introduced the concept of the \stress{span} of some
vectors.  Given finitely many vectors $\vv_1, \vv_2, \ldots, \vv_m$,
we define their \emph{span} to be the set of \emph{all linear
combinations} of these vectors.  We write
$$
\spn\{ \vv_1, \vv_2, \ldots, \vv_m\} = \{ a_1\vv_1 + a_2\vv_2 + \cdots + a_m\vv_m \st a_i \in \R\}.
$$

Thus far, we should agree that:
\begin{itemize}
\item $\spn\{\zero\} = \{ \zero\}$
\item If $\vv \neq \zero$, then $\spn\{ \vv \}$ is the line through the
origin in direction $\vv$.
\item If $\uu, \vv \neq \zero$ and $\uu$ and $\vv$ are not parallel, then
$\spn\{\uu,\vv\}$ is a plane (at least: in $\R^2$ and $\R^3$).
\end{itemize}
(We also argued that three non-coplanar vectors should span all of $\R^3$,
but this was only done verbally, and I don't expect 
you to feel convinced of that one, yet.)

Notice the (geometric!) conditions we had to place on our statements.  For now,
let's agree that they are necessary, and then work out what the
correct algebraic version of these conditions should be, so that
we can apply it to \emph{any} vector space.

(We'll call the condition
\stress{linear independence} and one thing it will guarantee, as we'll
see, is that if $\{ \vv_1, \vv_2, \ldots, \vv_m\}$ is a linearly
independent set, then the vector space $\spn\{\vv_1, \vv_2, \ldots, \vv_m\}$ is 
strictly \emph{bigger} than the vector space $\spn\{\vv_1, \vv_2, \ldots, \vv_{m-1}\}$
(or the span of any proper subset).)

\section[Difficulties with span, I]{Difficulties with span: Nonuniqueness of spanning sets}

The first issue to mention here is not one we're going to be able to
solve; but we need to discuss it to be aware of the issue.  

\begin{myexample} $\spn\{ (1,2)\} = \spn\{(2,4)\}$ since these vectors are 
parallel and so span the same line. \end{myexample}

\begin{myexample} If $W = \spn\{ (1,0,1), (0,1,0)\}$ and $U = \spn\{ (1,1,1), (1,-1,1)\}$
then in fact $W=U$.  Why? 

First note that $(1,1,1) = (1,0,1)+ (0,1,0)$ and $(1,-1,1)=(1,0,1)-(0,1,0)$.
Now apply Theorem~\ref{span}:

The set $W = \spn\{ (1,0,1), (0,1,0)\}$
is a subspace of $\R^3$ by Theorem~\ref{span}(1).\\  
We've just shown that
$(1,1,1)$ and $(1,-1,1)$ are in $W$ (because they are linear combinations 
of a spanning set for $W$).\\
Thus by part (2) of the theorem, $\spn\{  (1,1,1), (1,-1,1)\} \subseteq W$.\\
So $U \subseteq W$.

Next, write $(1,0,1) = \frac12(1,1,1)+\frac12(1,-1,1)$ and
$(0,1,0) = \frac12(1,1,1)-\frac12(1,-1,1)$, and apply the same
argument as above, with the roles of $U$ and $W$ reversed, to
deduce that $W \subseteq U$.

Hence $W=U$.
\end{myexample}

\standout{Moral:  You can't usually tell if two subspaces are
equal just by looking at their spanning sets!  Every vector space (except
the zero vector space) has \emph{infinitely many} spanning sets.}

\section[Difficulties with span, II]{Difficulties with span:  more vectors in the spanning set doesn't mean more vectors in their span}

The second issue is more serious, but it is one that we can (and will)
repair.

\begin{myprob} Show that
$$
\spn\{(1,2)\} = \spn\{ (0,0), (1,2) \} = \spn\{(1,2), (2,4), (3,6)\}.
$$

\begin{mysol} 
For the first equality, note that:
$$
\spn\{ (0,0), (1,2) \} = \{ a(0,0) + b(1,2) \st a,b\in \R\} = \{  b(1,2) \st b\in \R\} = \spn\{(1,2)\}.
$$
Also, note that
\begin{align*}
\spn\{(1,2), (2,4), (3,6)\} &= \{ a(1,2) + b(2,4) + c(3,6) \st a,b,c\in\R\}\\
&= \{ a(1,2) + 2b(1,2) + 3c(1,2) \st a,b,c\in \R\}\\
&= \{ (a+2b+3c)(1,2) \st  a,b,c\in \R\}
\end{align*}
This set is certainly contained in $\spn\{(1,2)\}$.  
Now, for any $t\in \R$, you could set $a=t, b=0, c=0$, to get $t(1,2)$
in this set.  So we have
$$
\spn\{(1,2), (2,4), (3,6)\} = \spn\{(1,2)\}
$$
\end{mysol}\end{myprob}

\standout{Moral:  The number of elements in the spanning set for a subspace $W$ does not necessarily tell you how ``big'' $W$ is.}

Geometrically, we see that the problem is that all the vectors are
collinear (meaning, parallel, or all lying on one line).  Similar
problems would occur in $\R^3$ if we had three \defn{coplanar} vectors,
that is, all lying in a plane.  So what is the algebraic analogue
of these statements?

\section{Algebraic version of ``2 vectors are collinear''}

Two vectors $\uu$ and $\vv$ being \defn{collinear}, or parallel, means that 
either they are multiples of one another or one of them is $\zero$.

So it's not enough to just say $\uu = k \vv$ for some $k\in \R$ (because
maybe $\vv = \zero$ and then this equation wouldn't work).

You could go with:
\begin{itemize}
\item $\uu = k \vv$ for some $k\in \R$, or
\item $\vv = k \uu$ for some $k\in \R$.
\end{itemize}
But here's a nice compact way to get all the cases at once:

\standout{Two vectors $\uu$ and $\vv$ are collinear if
there exist scalars $a, b\in \R$, \emph{not both zero}, such that
$$
a\uu + b\vv = \zero
$$}

Since the coefficients aren't both zero, you can rearrange this
as above. 

\begin{myexample}
The vectors $(1,2,1)$ and $(2,4,2)$ are collinear since
$$
2(1,2,1) + (-1)(2,4,2) = (0,0,0).
$$\end{myexample}

\begin{myexample} The vectors $(1,2,1)$ and $(0,0,0)$ are collinear since
$$
0(1,2,1) + 3(0,0,0)= (0,0,0)
$$
(note that at least one of the coefficients isn't zero). \end{myexample}

\section{Algebraic version of ``3 vectors are coplanar''}

Again, we could think of this as:  one of the three vectors
is in the span of the other two.  So if the vectors are
$\uu$, $\vv$ and $\ww$, they are coplanar if
\begin{itemize}
\item $\uu = a\vv+b\ww$ for some $a,b\in \R$ or
\item $\vv = a\uu + b\ww$ for some  $a,b\in \R$ or
\item $\ww = a\uu + b\vv$ for some  $a,b\in \R$.
\end{itemize}
(Where again we had to include all three cases just in case
one of the vectors is the zero vector.)

By the same method as above, this can be simplified to:

\standout{Three vectors $\uu$, $\vv$ and $\ww$ are
coplanar (lie in a plane) if there exists scalars $a,b,c \in \R$,
\emph{not all zero} such that
$$
a\uu + b\vv + c\ww = \zero
$$
}

\begin{myexample} $(1,0,1), (0,1,0), (1,1,1)$ are coplanar because
$$
(1,0,1) + (0,1,0) - (1,1,1) = (0,0,0).
$$
\end{myexample}

\begin{myexample} $(1,1,1), (2,2,2), (0,0,0)$ are coplanar because
$$
2(1,1,1) + (-1)(2,2,2)+0(0,0,0) = (0,0,0)
$$
or because
$$
0(1,1,1) + 0(2,2,2) + 17(0,0,0) = (0,0,0)
$$
(in fact these vectors were even collinear, but let's be happy
with coplanar for now).\end{myexample}

\section[Linear DEPENDENCE]{Linear DEPENDENCE: the algebraic generalization of ``collinear'' and ``coplanar''}



\begin{definition}  Let $V$ be a vector space and
$\vv_1, \vv_2, \ldots, \vv_m \in V$.  Then the set 
$\{\vv_1, \vv_2, \ldots, \vv_m\}$ is \emph{linearly dependent}\index{linearly dependent (LD)}
(or say:  $\vv_1, \vv_2, \ldots, \vv_m$ \emph{are} linearly dependent)
if and only if there are scalars $a_1, a_2, \ldots, a_m \in \R$,
\stress{not all zero} such that 
$$
a_1\vv_1 + \cdots + a_m\vv_m = \zero.
$$
\end{definition}

This (and the definition of linear independence, coming up next) are \stress{key concepts} for this course.  Take the time to note what the definition does \stress{not} say:  it doesn't just say that there's a solution to the \defn{dependence equation} $a_1\vv_1 + \cdots + a_m\vv_m = \zero$; it says there is a \stress{nontrivial solution} to it.  That is a huge difference.

\section{The contrapositive and linear INDEPENDENCE}

So linear dependence is a concept (valid for any number of vectors, in any vector space, of any dimension) that captures the idea that our set of vectors has some ``redundancy", such as being collinear or coplanar.  The definition is quite complicated, so it is worth asking: so, what is the \stress{opposite} of a set being linearly dependent?
 
Rephrasing what we have above, we can say:
\begin{itemize}
\item Two vectors $\uu$ and $\vv$ are not collinear (equivalently, 
$\uu$ and $\vv$ are not parallel) if and only if
$$
a\uu + b\vv = \zero \Leftrightarrow a=b=0
$$
In words:  they are not parallel if and only the ONLY solution to
the equation $a\uu + b\vv = \zero$ is the \emph{trivial} solution
$a=b=0$ (which of course is always a solution).

\item Three vectors $\uu$, $\vv$, $\ww$ are not coplanar if and only if
$$
a\uu + b\vv + c\ww = \zero \Leftrightarrow a=b=c=0
$$
In words:  they are not coplanar if and only if the ONLY solution to
the equation $a\uu + b\vv + c\ww = \zero$ is the trivial solution $a=b=c=0$.
\end{itemize}


\begin{definition}  Let $V$ be a vector space and
$\vv_1, \vv_2, \ldots, \vv_m \in V$.  Then the set 
$\{\vv_1, \vv_2, \ldots, \vv_m\}$ is \emph{linearly independent}\index{linearly independent (LI)}
(or say:  $\vv_1, \vv_2, \ldots, \vv_m$ \emph{are} linearly independent)
if and only if the \emph{only solution to}
$$
a_1\vv_1 + \cdots + a_m\vv_m = \zero
$$
is the trivial solution $a_1=0$, $\cdots$, $a_m=0$.
\end{definition}

\standout{Note that if some vectors are not linearly dependent, then they are linearly independent, and vice versa.}

We will abbreviate LI = linearly independent and LD = linearly dependent.

\section{Examples}

Let's work with this definition in several examples.

\begin{myexample}\label{R2} $\{(1,0), (0,1)\}$ is LI because when you try to solve 
$$
a(1,0)+b(0,1) = (0,0)
$$
you get 
$$
(a,b) = (0,0)
$$
which means $a=0, b=0$.  No choice! \end{myexample}

\begin{myexample} $\{(1,1), (1,-1)\}$ is LI because when you try to solve
$$
a(1,1)+b(1,-1) = (0,0)
$$
you get
$$
(a+b,a-b)= (0,0)
$$
which gives $a=b$ and $a=-b$, which forces $a=0$ and $b=0$,  No choice! \end{myexample}

\begin{myexample} $\{(1,0), (0,1), (1,1)\}$ is LD since
when you try to solve
$$
a(1,0) + b(0,1) + c(1,1) = (0,0)
$$
you notice that $a=1$, $b=1$ and $c=-1$ works.  (In fact, you have
infinitely many nontrivial solutions!)  Since we found a nontrivial
solution, they are dependent.  (Note that $(1,1) \in \spn\{(1,0),(0,1)\}$.)
\end{myexample}

\begin{myexample} $\{ (1,1,1)\}$ is LI since solving $k(1,1,1)=\zero$ forces $k=0$. \end{myexample}

\begin{myexample} $\left\{ \mat{1&0\\0&0}, \mat{0&1\\0&0},  \mat{0&0\\1&0}, \mat{0&0\\0&1} \right\}$ is LI since when you try to solve
$$
a_1\mat{1&0\\0&0}+a_2\mat{0&1\\0&0}+a_3\mat{0&0\\1&0}+a_4 \mat{0&0\\0&1} =
 \mat{0&0\\0&0}
$$
you get, by simplifying the left side,
$$
\mat{a_1 & a_2 \\ a_3 & a_4} =  \mat{0&0\\0&0}
$$
which means all your coefficients $a_i$ are forced to be zero.\end{myexample}

\begin{myexample}  $\left\{ \mat{1&0\\0&0}, \mat{0&0\\0&1},  \mat{1&0\\0&-1}, \mat{0&1\\0&0} \right\}$ is LD since we notice that
$$
\mat{1&0\\0&0} - \mat{0&0\\0&1} - \mat{1&0\\0&-1} + 0\mat{0&1\\0&0} = \mat{0&0\\0&0}.
$$
Note that the third matrix was in the span of the first two. \end{myexample}

\begin{myexample}\label{imp} $\{ 1,x,x^2\}$ is LI in $\PP_2$ since
$$
a(1) + b(x) + c(x^2) = \zero 
$$
means that $$
a  + bx + c x^2  = 0 
$$ for {\it all real values of $x$}. If $c\not=0$, the left hand side is a is a quadratic polynomial which can at at most two distinct real roots! So $c=0$. Then we have $$
a  + bx  = 0 
$$ for {\it all real values of $x$.} But if $b\not=0$, this equation has the single solution $x=-\dfrac{a}{b}$! So $b$ must be zero. Now we see that $a=0$. Hence $a=b=c=0$.\footnote{Compare this argument with the hints given for problems \ref{prob06.4} parts (e) and (f).} \end{myexample}

\begin{myexample} $\{ 4+4x+x^2, 1+x, x^2\}$ is LD in $\PP_2$ since
$$
1(4+4x+x^2) +(-4)(1+x) + (-1)x^2 = 0
$$
is a nontrivial dependence equation.  In fact, $4+4x+x^2 \in \spn\{ 1+x, x^2\}$.
\end{myexample}

\begin{myexample} $\{ 1, \sin x, \cos x\}$ is LI in $F(\R)$ because suppose we
have the equation
$$
a(1) + b\sin(x) + c\cos(x) = 0
$$
then, plugging in some values of $x$ gives you:
\begin{itemize}
\item $x=0$ : $a+c = 0$
\item $x=\pi/2$ : $a+b = 0$
\item $x=\pi$ : $a-c = 0$
\end{itemize}
And we see from this (with a little bit of work) that $a=b=c=0$. \end{myexample}


\begin{myexample} $\{1, \sin^2 x, \cos^2 x\}$ is LD.  Why?  Since
$$
(-1) 1 + (1) \sin^2 x + (1) \cos^2 x = 0,  \quad \textrm{for all $x\in \R$},
$$
this is a nontrivial dependence relation.  Note that $\cos^2 x \in \spn\{1,\sin^2 x\}$.
\end{myexample}

\section[Facts about linear dependence and linear independence]{Facts (theorems) about linear independence and linear dependence}
Let's see how many general facts we can figure out about LI and LD sets. Let $V$ be a vector space.


\begin{fac}1:  If $\vv \in V$, then $\{ \vv \}$ is linearly independent if and only if $\vv \neq \zero$.  \end{fac}

\begin{proof}  if $\vv \neq \zero$, then $k\vv = \zero$ only has solution $k=0$ (LI);
but if $\vv = \zero$, then we have, for example, $3\vv = \zero$ (LD). \end{proof}


\begin{fac} 2:  If $\{\vv_1, \dots, \vv_m\}$ is LD, then any set containing
$\{\vv_1, \dots, \vv_m\}$ is also LD.\end{fac}

Point: you can't ``fix'' a dependent set by adding more vectors to it.
In $\R^3$, if 2 vectors are collinear, then taking a third vector gives you at most a
coplanar set.

\begin{proof} Your set is LD so 
you have a nontrivial dependence relation
$$
a_1\vv_1 + \cdots + a_m \vv_m = \zero
$$ 
with not all $a_i = 0$.  Now consider the bigger set $\{ \vv_1, \ldots, \vv_m, \uu_1, \dots, \uu_k\}$.  
Then you have the equation
$$
 a_1\vv_1 + \cdots + a_m \vv_m + 0\uu_{1} + \cdots + 0\uu_k = \zero
$$
and not all coefficients are zero.  So that means the big set is
LD, too. \end{proof}

\begin{fac} 3: If $\{ \vv_1, \cdots, \vv_m\}$ is LI, then any subset is also LI.\end{fac}

\begin{myexample} If three vectors are not coplanar, then for sure, no pair of them is
collinear.\end{myexample}

\begin{proof} Fact 3 is in fact logically equivalent to Fact 2.  
 Fact 2 says that if you contain an LD subset,  then you are LD.
So you can't be LI and contain an LD subset.
\end{proof}

\begin{fac} 4: $\{ \zero\}$ is LD.\end{fac}

\begin{fac} 5: Any set containing the zero vector is LD.\end{fac}

\standout{NOTICE: this means that subspaces are LD!  And that's perfectly OK.  
We are only usually interested in whether or not
\emph{spanning sets} are LD or LI.}

\begin{proof} Use Fact 4 with Fact 2. \end{proof}

\begin{fac} 6: A set $\{\uu,\vv\}$ is LD if and only if one of the
vectors is a multiple of the other.\end{fac}

\begin{proof}  If $a\uu + b\vv = \zero$ is a nontrivial dependence
relation and $a\neq 0$, then $\uu = -\dfrac{b}{a} \vv$.  If $b\neq 0$
then $\vv =-\dfrac{a}{b} \uu$.  (And if both are zero, it's not a nontrivial
dependence relation!) Conversely, if $\uu= c \vv$ (or $\vv=a\uu$) for some $c\in \R$, then $\uu-c\vv=0$  (or $-c\uu+\vv=0$) is a nontrivial
dependence relation. \end{proof}

\begin{fac} 7: A set with three or more vectors can be LD \emph{even though} no two vectors are
multiples of one another.\end{fac}

\begin{myexample}  $\{(1,0), (0,1), (1,1)\}$ are coplanar but no two vectors
are collinear. \end{myexample}

\begin{fac} 8: (See Theorem \ref{depspan} in the next chapter)   
A set $\{\vv_1, \dots, \vv_m\}$ is LD if and only if there is at least
one vector $\vv_k \in \{\vv_1, \dots, \vv_m\}$  which is in the span of $\{\vv_1, \dots, \vv_{k-1}, \vv_{k+1}, \dots,\vv_m\}$.\end{fac}

\begin{myexample} (using the above example)  $(1,1) \in \spn\{(1,0), (0,1)\}$.\end{myexample}

\standout{Caution:  Fact 8 \emph{doesn't mean} that \emph{every} vector is a
linear combination of the others.  For example, 
$\{ (1,1), (2,2), (1,3)\}$ is LD BUT $(1,3) \notin \spn\{(1,1),(2,2)\}$.}

\begin{proof} Suppose $\{\vv_1, \cdots, \vv_m\}$ is LD.  So there is
some nontrivial dependence equation
$$
a_1\vv_1 + \cdots + a_m\vv_m = \zero
$$
with not all $a_i = 0$.  Let's say that $a_1 \neq 0$ (we could always
renumber the vectors so that this is the case).  Then we can solve for
$\vv_1$:
$$
\vv_1 = -\frac{a_2}{a_1}\vv_2 - \cdots - \frac{a_m}{a_1}\vv_m
$$
which just says $\vv_1 \in \spn\{ \vv_2, \cdots, \vv_m\}$.  Done.

Now the other direction.  Suppose $\vv_m \in \spn\{ \vv_1, \cdots, \vv_{m-1}\}$.
That means $\vv_m = b_1\vv_1 + \cdots + b_{n-1}\vv_{m-1}$.  So we have
$$
b_1\vv_1 + \cdots + b_{n-1}\vv_{m-1} + (-1)\vv_m = \zero
$$ 
and the coefficient of $\vv_n$ is $-1$, which is nonzero.  Hence this
is a nontrivial dependence relation (it doesn't even matter if all
the $b_i$ were zero).  Thus the set is LD.
\end{proof}

Next chapter, we'll put these ideas together with spanning sets.





